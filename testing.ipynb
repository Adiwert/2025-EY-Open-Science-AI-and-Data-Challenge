{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# # Urban Heat Island Prediction with Satellite Data\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ## Optimized Pipeline with Gradient Boosting\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# %% [code]\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Mount Google Drive\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      8\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m, force_remount\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# %% [code]\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Import libraries\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Urban Heat Island Prediction with Satellite Data\n",
    "# ## Optimized Pipeline with Gradient Boosting\n",
    "\n",
    "# %% [code]\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# %% [code]\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import rasterio\n",
    "from rasterio import transform\n",
    "from pystac_client import Client\n",
    "import planetary_computer\n",
    "from shapely.geometry import Polygon\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# %% [code]\n",
    "# File paths\n",
    "TRAIN_PATH = '/content/drive/MyDrive/Colab Notebooks/Training_data_uhi_index_UHI2025-v3.csv'\n",
    "TEST_PATH = '/content/drive/MyDrive/Colab Notebooks/Submission_template_UHI2025-v3.csv'\n",
    "\n",
    "# %% [code]\n",
    "# Load data\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# Convert datetime\n",
    "def convert_date(date_str):\n",
    "    return datetime.strptime(date_str[:10], \"%d-%m-%Y\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "train_df['datetime'] = train_df['datetime'].apply(convert_date)\n",
    "\n",
    "# %% [code]\n",
    "# STAC API configuration\n",
    "STAC_API = \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
    "BANDS = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', \n",
    "         'B07', 'B08', 'B8A', 'B11', 'B12']\n",
    "\n",
    "# %% [code]\n",
    "def fetch_satellite_data(df):\n",
    "    \"\"\"\n",
    "    Fetch satellite data for all points in dataframe\n",
    "    Returns xarray Dataset with all bands\n",
    "    \"\"\"\n",
    "    catalog = Client.open(STAC_API)\n",
    "    \n",
    "    all_data = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # Create point geometry\n",
    "        geometry = Polygon.from_bounds(\n",
    "            row['Longitude'], row['Latitude'],\n",
    "            row['Longitude'], row['Latitude']\n",
    "        )\n",
    "        \n",
    "        # Search for items\n",
    "        search = catalog.search(\n",
    "            collections=[\"sentinel-2-l2a\"],\n",
    "            datetime=row['datetime'],\n",
    "            query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
    "            intersects=geometry\n",
    "        )\n",
    "        \n",
    "        items = list(search.get_items())\n",
    "        if not items:\n",
    "            continue\n",
    "            \n",
    "        # Take the least cloudy scene\n",
    "        best_item = min(items, key=lambda x: x.properties['eo:cloud_cover'])\n",
    "        signed_item = planetary_computer.sign(best_item)\n",
    "        \n",
    "        # Load data\n",
    "        data = stac_load(\n",
    "            [signed_item],\n",
    "            bands=BANDS,\n",
    "            crs=\"EPSG:4326\",\n",
    "            resolution=0.00027,  # ~10m resolution\n",
    "            chunks={},\n",
    "            bbox=[row['Longitude'], row['Latitude'], row['Longitude'], row['Latitude']]\n",
    "        ).compute()\n",
    "        \n",
    "        all_data.append(data)\n",
    "    \n",
    "    return xr.concat(all_data, dim='time')\n",
    "\n",
    "# %% [code]\n",
    "# Fetch training satellite data\n",
    "print(\"Fetching training satellite data...\")\n",
    "train_sat_data = fetch_satellite_data(train_df)\n",
    "\n",
    "# %% [code]\n",
    "# Feature engineering functions\n",
    "def calculate_indices(ds):\n",
    "    \"\"\"Calculate spectral indices from satellite data\"\"\"\n",
    "    ds['NDVI'] = (ds.B08 - ds.B04) / (ds.B08 + ds.B04)\n",
    "    ds['NDBI'] = (ds.B11 - ds.B08) / (ds.B11 + ds.B08)\n",
    "    ds['NDWI'] = (ds.B03 - ds.B08) / (ds.B03 + ds.B08)\n",
    "    ds['SAVI'] = 1.5 * (ds.B08 - ds.B04) / (ds.B08 + ds.B04 + 0.5)\n",
    "    ds['MSAVI'] = (2 * ds.B08 + 1 - \n",
    "                  np.sqrt((2 * ds.B08 + 1)**2 - 8 * (ds.B08 - ds.B04))) / 2\n",
    "    ds['UI'] = (ds.B12 - ds.B08) / (ds.B12 + ds.B08)\n",
    "    return ds\n",
    "\n",
    "# %% [code]\n",
    "# Process training data\n",
    "print(\"Processing training data...\")\n",
    "train_sat_data = calculate_indices(train_sat_data)\n",
    "features = train_sat_data.to_dataframe().reset_index()\n",
    "\n",
    "# Merge with original dataframe\n",
    "full_train_df = pd.merge(\n",
    "    train_df,\n",
    "    features,\n",
    "    left_on=['datetime', 'Latitude', 'Longitude'],\n",
    "    right_on=['time', 'latitude', 'longitude']\n",
    ")\n",
    "\n",
    "# Clean data\n",
    "full_train_df = full_train_df.dropna()\n",
    "X = full_train_df.drop(columns=['UHI Index', 'datetime', 'time', 'latitude', 'longitude'])\n",
    "y = full_train_df['UHI Index']\n",
    "\n",
    "# %% [code]\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# %% [code]\n",
    "# Hyperparameter tuning\n",
    "params = {\n",
    "    'n_estimators': 500,\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 7,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.9,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1\n",
    "}\n",
    "\n",
    "# Initialize and train model\n",
    "model = lgb.LGBMRegressor(**params)\n",
    "model.fit(X_train_scaled, y_train,\n",
    "          eval_set=[(X_test_scaled, y_test)],\n",
    "          early_stopping_rounds=50,\n",
    "          verbose=100)\n",
    "\n",
    "# Evaluate model\n",
    "train_pred = model.predict(X_train_scaled)\n",
    "test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(f\"Train R²: {r2_score(y_train, train_pred):.4f}\")\n",
    "print(f\"Test R²: {r2_score(y_test, test_pred):.4f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(model, X_train_scaled, y_train, \n",
    "                            cv=5, scoring='r2')\n",
    "print(f\"Cross-validation R²: {np.mean(cv_scores):.4f} ± {np.std(cv_scores):.4f}\")\n",
    "\n",
    "# %% [code]\n",
    "# Process test data\n",
    "print(\"Processing test data...\")\n",
    "test_sat_data = fetch_satellite_data(test_df)\n",
    "test_sat_data = calculate_indices(test_sat_data)\n",
    "test_features = test_sat_data.to_dataframe().reset_index()\n",
    "\n",
    "# Prepare submission\n",
    "test_scaled = scaler.transform(test_features[X.columns])\n",
    "test_pred = model.predict(test_scaled)\n",
    "\n",
    "# Create submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'Longitude': test_df['Longitude'],\n",
    "    'Latitude': test_df['Latitude'],\n",
    "    'UHI Index': test_pred\n",
    "})\n",
    "\n",
    "# Save results\n",
    "submission_df.to_csv('/content/drive/MyDrive/Colab Notebooks/UHI_Final_Submission.csv', index=False)\n",
    "print(\"Submission file saved!\")\n",
    "\n",
    "# %% [code]\n",
    "# Feature importance visualization\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
